

Splink is a record linkage library.  It's recently been updated from version 3 to version 4.

Your job is to upgrade code from version 3 to Splink 4.  In what follows I will give you a big diff that shows how a bunch of scripts were upgraded to use Splink 4.  Use these as the basis for any instructions

diff --git a/notebooks_as_py/02_Exploratory_analysis.py b/notebooks_as_py/02_Exploratory_analysis.py
index 84125f7..2e19f43 100644
--- a/notebooks_as_py/02_Exploratory_analysis.py
+++ b/notebooks_as_py/02_Exploratory_analysis.py
@@ -1,13 +1,17 @@
-from splink.datasets import splink_datasets
-import altair as alt
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import  splink_datasets

 df = splink_datasets.fake_1000
+df = df.drop(columns=["cluster"])
 df.head(5)

-# Initialise the linker, passing in the input dataset(s)
-from splink.duckdb.linker import DuckDBLinker
-linker = DuckDBLinker(df)
+from splink.exploratory import completeness_chart
+from splink import DuckDBAPI
+db_api = DuckDBAPI()
+completeness_chart(df, db_api=db_api)

-linker.missingness_chart()
+from splink.exploratory import profile_columns

-linker.profile_columns(top_n=10, bottom_n=5)
\ No newline at end of file
+profile_columns(df, db_api=DuckDBAPI(), top_n=10, bottom_n=5)
\ No newline at end of file
diff --git a/notebooks_as_py/03_Blocking.py b/notebooks_as_py/03_Blocking.py
index 05fffb0..0617bff 100644
--- a/notebooks_as_py/03_Blocking.py
+++ b/notebooks_as_py/03_Blocking.py
@@ -1,31 +1,66 @@
-from splink.datasets import splink_datasets
-import altair as alt
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import DuckDBAPI, block_on, splink_datasets

 df = splink_datasets.fake_1000

-from splink.duckdb.linker import DuckDBLinker
-from splink.duckdb.blocking_rule_library import block_on
-settings = {"link_type": "dedupe_only"}
-linker = DuckDBLinker(df, settings)
+from splink.blocking_analysis import count_comparisons_from_blocking_rule
+
+db_api = DuckDBAPI()
+
+br = block_on("substr(first_name, 1,1)", "surname")
+
+counts = count_comparisons_from_blocking_rule(
+    table_or_tables=df,
+    blocking_rule=br,
+    link_type="dedupe_only",
+    db_api=db_api,
+)
+
+counts
+
+br = "l.first_name = r.first_name and levenshtein(l.surname, r.surname) < 2"
+
+counts = count_comparisons_from_blocking_rule(
+    table_or_tables=df,
+    blocking_rule= br,
+    link_type="dedupe_only",
+    db_api=db_api,
+)
+counts
+
+from splink.blocking_analysis import n_largest_blocks
+
+result = n_largest_blocks(    table_or_tables=df,
+    blocking_rule= block_on("city", "first_name"),
+    link_type="dedupe_only",
+    db_api=db_api,
+    n_largest=3
+    )

-blocking_rule_1 = block_on(["substr(first_name, 1,1)", "surname"])
-count = linker.count_num_comparisons_from_blocking_rule(blocking_rule_1)
-print(f"Number of comparisons generated by '{blocking_rule_1.blocking_rule_sql}': {count:,.0f}")
+result.as_pandas_dataframe()

-blocking_rule_2 = block_on("surname")
-count = linker.count_num_comparisons_from_blocking_rule(blocking_rule_2)
-print(f"Number of comparisons generated by '{blocking_rule_2.blocking_rule_sql}': {count:,.0f}")
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)

-blocking_rule_3 = block_on("email")
-count = linker.count_num_comparisons_from_blocking_rule(blocking_rule_3)
-print(f"Number of comparisons generated by '{blocking_rule_3.blocking_rule_sql}': {count:,.0f}")
+blocking_rules_for_analysis = [
+    block_on("substr(first_name, 1,1)", "surname"),
+    block_on("surname"),
+    block_on("email"),
+    block_on("city", "first_name"),
+    "l.first_name = r.first_name and levenshtein(l.surname, r.surname) < 2",
+]

-blocking_rule_4 = block_on(["city", "first_name"])
-count = linker.count_num_comparisons_from_blocking_rule(blocking_rule_4)
-print(f"Number of comparisons generated by '{blocking_rule_4.blocking_rule_sql}': {count:,.0f}")

+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=df,
+    blocking_rules=blocking_rules_for_analysis,
+    db_api=db_api,
+    link_type="dedupe_only",
+)

-blocking_rules = [blocking_rule_1, blocking_rule_2, blocking_rule_3]
-linker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)
+from splink.exploratory import profile_columns

-linker.profile_columns("city || left(first_name,1)")
\ No newline at end of file
+profile_columns(df, column_expressions=["city || left(first_name,1)"], db_api=db_api)
\ No newline at end of file
diff --git a/notebooks_as_py/04_Estimating_model_parameters.py b/notebooks_as_py/04_Estimating_model_parameters.py
index 21e260c..f322603 100644
--- a/notebooks_as_py/04_Estimating_model_parameters.py
+++ b/notebooks_as_py/04_Estimating_model_parameters.py
@@ -1,65 +1,68 @@
-# Begin by reading in the tutorial data again
-from splink.duckdb.linker import DuckDBLinker
-from splink.datasets import splink_datasets
-import altair as alt
-df = splink_datasets.fake_1000
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink

-import splink.duckdb.comparison_library as cl
+# Begin by reading in the tutorial data again
+from splink import splink_datasets

-email_comparison =  cl.levenshtein_at_thresholds("email", 2)
-print(email_comparison.human_readable_description)
+df = splink_datasets.fake_1000

-import splink.duckdb.comparison_template_library as ctl
+import splink.comparison_library as cl

-first_name_comparison = ctl.name_comparison("first_name")
-print(first_name_comparison.human_readable_description)
+city_comparison = cl.LevenshteinAtThresholds("city", 2)
+print(city_comparison.get_comparison("duckdb").human_readable_description)

+email_comparison = cl.EmailComparison("email")
+print(email_comparison.get_comparison("duckdb").human_readable_description)

-from splink.duckdb.blocking_rule_library import block_on
+from splink import Linker, SettingsCreator, block_on, DuckDBAPI

-settings = {
-    "link_type": "dedupe_only",
-    "comparisons": [
-        ctl.name_comparison("first_name"),
-        ctl.name_comparison("surname"),
-        ctl.date_comparison("dob", cast_strings_to_date=True),
-        cl.exact_match("city", term_frequency_adjustments=True),
-        ctl.email_comparison("email", include_username_fuzzy_level=False),
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    comparisons=[
+        cl.NameComparison("first_name"),
+        cl.NameComparison("surname"),
+        cl.LevenshteinAtThresholds("dob", 1),
+        cl.ExactMatch("city").configure(term_frequency_adjustments=True),
+        cl.EmailComparison("email"),
     ],
-    "blocking_rules_to_generate_predictions": [
-        block_on("first_name"),
+    blocking_rules_to_generate_predictions=[
+        block_on("first_name", "city"),
         block_on("surname"),
+
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)

-linker = DuckDBLinker(df, settings)
+linker = Linker(df, settings, db_api=DuckDBAPI())

 deterministic_rules = [
-    "l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1",
-    "l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1",
+    block_on("first_name", "dob"),
     "l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2",
-    "l.email = r.email"
+    block_on("email")
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)
-
-linker.estimate_u_using_random_sampling(max_pairs=1e6)
+linker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)

-training_blocking_rule = block_on(["first_name", "surname"])
-training_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
-
-from numpy import fix
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6)

+training_blocking_rule = block_on("first_name", "surname")
+training_session_fname_sname = (
+    linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+)

 training_blocking_rule = block_on("dob")
-training_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+training_session_dob = linker.training.estimate_parameters_using_expectation_maximisation(
+    training_blocking_rule
+)
+
+linker.visualisations.match_weights_chart()

-linker.match_weights_chart()
+linker.visualisations.m_u_parameters_chart()

-linker.m_u_parameters_chart()
+linker.visualisations.parameter_estimate_comparisons_chart()

-settings = linker.save_model_to_json("../demo_settings/saved_model_from_demo.json", overwrite=True)
+settings = linker.misc.save_model_to_json(
+    "../demo_settings/saved_model_from_demo.json", overwrite=True
+)

-linker.unlinkables_chart()
\ No newline at end of file
+linker.evaluation.unlinkables_chart()
\ No newline at end of file
diff --git a/notebooks_as_py/05_Predicting_results.py b/notebooks_as_py/05_Predicting_results.py
index 3ada8f8..08866be 100644
--- a/notebooks_as_py/05_Predicting_results.py
+++ b/notebooks_as_py/05_Predicting_results.py
@@ -1,21 +1,37 @@
-from splink.duckdb.linker import DuckDBLinker
-from splink.datasets import splink_datasets
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import Linker, DuckDBAPI, splink_datasets
+
 import pandas as pd
+
 pd.options.display.max_columns = 1000
+
+db_api = DuckDBAPI()
 df = splink_datasets.fake_1000

-linker = DuckDBLinker(df)
-linker.load_model("../demo_settings/saved_model_from_demo.json")
+import json
+import urllib
+
+url = "https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json"
+
+with urllib.request.urlopen(url) as u:
+    settings = json.loads(u.read().decode())
+
+
+linker = Linker(df, settings, db_api=DuckDBAPI())

-df_predictions = linker.predict(threshold_match_probability=0.2)
+df_predictions = linker.inference.predict(threshold_match_probability=0.2)
 df_predictions.as_pandas_dataframe(limit=5)

-clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)
+clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(
+    df_predictions, threshold_match_probability=0.5
+)
 clusters.as_pandas_dataframe(limit=10)

 sql = f"""
-select *
+select *
 from {df_predictions.physical_name}
 limit 2
 """
-linker.query_sql(sql)
\ No newline at end of file
+linker.misc.query_sql(sql)
\ No newline at end of file
diff --git a/notebooks_as_py/06_Visualising_predictions.py b/notebooks_as_py/06_Visualising_predictions.py
index a2259b2..f67c931 100644
--- a/notebooks_as_py/06_Visualising_predictions.py
+++ b/notebooks_as_py/06_Visualising_predictions.py
@@ -1,31 +1,51 @@
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
 # Rerun our predictions to we're ready to view the charts
-from splink.duckdb.linker import DuckDBLinker
-from splink.datasets import splink_datasets
-import altair as alt
+from splink import Linker, DuckDBAPI, splink_datasets
+
+import pandas as pd
+
+pd.options.display.max_columns = 1000

+db_api = DuckDBAPI()
 df = splink_datasets.fake_1000
-linker = DuckDBLinker(df)
-linker.load_model("../demo_settings/saved_model_from_demo.json")
-df_predictions = linker.predict(threshold_match_probability=0.2)

-records_to_view  = df_predictions.as_record_dict(limit=5)
-linker.waterfall_chart(records_to_view, filter_nulls=False)
+import json
+import urllib
+
+url = "https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json"
+
+with urllib.request.urlopen(url) as u:
+    settings = json.loads(u.read().decode())


-linker.comparison_viewer_dashboard(df_predictions, "scv.html", overwrite=True)
+linker = Linker(df, settings, db_api=DuckDBAPI())
+df_predictions = linker.inference.predict(threshold_match_probability=0.2)
+
+records_to_view = df_predictions.as_record_dict(limit=5)
+linker.visualisations.waterfall_chart(records_to_view, filter_nulls=False)
+
+linker.visualisations.comparison_viewer_dashboard(df_predictions, "scv.html", overwrite=True)

 # You can view the scv.html file in your browser, or inline in a notbook as follows
 from IPython.display import IFrame
-IFrame(
-    src="./scv.html", width="100%", height=1200
-)

-df_clusters = linker.cluster_pairwise_predictions_at_threshold(df_predictions, threshold_match_probability=0.5)
+IFrame(src="./scv.html", width="100%", height=1200)

-linker.cluster_studio_dashboard(df_predictions, df_clusters, "cluster_studio.html", sampling_method="by_cluster_size", overwrite=True)
+df_clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(
+    df_predictions, threshold_match_probability=0.5
+)
+
+linker.visualisations.cluster_studio_dashboard(
+    df_predictions,
+    df_clusters,
+    "cluster_studio.html",
+    sampling_method="by_cluster_size",
+    overwrite=True,
+)

 # You can view the scv.html file in your browser, or inline in a notbook as follows
 from IPython.display import IFrame
-IFrame(
-    src="./cluster_studio.html", width="100%", height=1200
-)
\ No newline at end of file
+
+IFrame(src="./cluster_studio.html", width="100%", height=1000)
\ No newline at end of file
diff --git a/notebooks_as_py/07_Evaluation.py b/notebooks_as_py/07_Evaluation.py
index c1d4907..96c7a81 100644
--- a/notebooks_as_py/07_Evaluation.py
+++ b/notebooks_as_py/07_Evaluation.py
@@ -1,23 +1,67 @@
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
 # Rerun our predictions to we're ready to view the charts
-from splink.duckdb.linker import DuckDBLinker
-from splink.datasets import splink_datasets
+import pandas as pd
+
+from splink import DuckDBAPI, Linker, splink_datasets

-import altair as alt
+pd.options.display.max_columns = 1000

+db_api = DuckDBAPI()
 df = splink_datasets.fake_1000
-linker = DuckDBLinker(df)
-linker.load_model("../demo_settings/saved_model_from_demo.json")
-df_predictions = linker.predict(threshold_match_probability=0.2)
+
+import json
+import urllib
+
+from splink import block_on
+
+url = "https://raw.githubusercontent.com/moj-analytical-services/splink/847e32508b1a9cdd7bcd2ca6c0a74e547fb69865/docs/demos/demo_settings/saved_model_from_demo.json"
+
+with urllib.request.urlopen(url) as u:
+    settings = json.loads(u.read().decode())
+
+# The data quality is very poor in this dataset, so we need looser blocking rules
+# to achieve decent recall
+settings["blocking_rules_to_generate_predictions"] = [
+    block_on("first_name"),
+    block_on("city"),
+    block_on("email"),
+    block_on("dob"),
+]
+
+linker = Linker(df, settings, db_api=DuckDBAPI())
+df_predictions = linker.inference.predict(threshold_match_probability=0.01)

 from splink.datasets import splink_dataset_labels

 df_labels = splink_dataset_labels.fake_1000_labels
+labels_table = linker.table_management.register_labels_table(df_labels)
 df_labels.head(5)
-labels_table = linker.register_labels_table(df_labels)

-linker.roc_chart_from_labels_table(labels_table)
+splink_df = linker.evaluation.prediction_errors_from_labels_table(
+    labels_table, include_false_negatives=True, include_false_positives=False
+)
+false_negatives = splink_df.as_record_dict(limit=5)
+linker.visualisations.waterfall_chart(false_negatives)
+
+# Note I've picked a threshold match probability of 0.01 here because otherwise
+# in this simple example there are no false positives
+splink_df = linker.evaluation.prediction_errors_from_labels_table(
+    labels_table, include_false_negatives=False, include_false_positives=True, threshold_match_probability=0.01
+)
+false_postives = splink_df.as_record_dict(limit=5)
+linker.visualisations.waterfall_chart(false_postives)
+
+linker.evaluation.accuracy_analysis_from_labels_table(
+    labels_table, output_type="threshold_selection", add_metrics=["f1"]
+)
+
+linker.evaluation.accuracy_analysis_from_labels_table(labels_table, output_type="roc")

-linker.precision_recall_chart_from_labels_table(labels_table)
+roc_table = linker.evaluation.accuracy_analysis_from_labels_table(
+    labels_table, output_type="table"
+)
+roc_table.as_pandas_dataframe(limit=5)

-roc_table = linker.truth_space_table_from_labels_table(labels_table)
-roc_table.as_pandas_dataframe(limit=5)
\ No newline at end of file
+linker.evaluation.unlinkables_chart()
\ No newline at end of file
diff --git a/notebooks_as_py/accuracy_analysis_from_labels_column.py b/notebooks_as_py/accuracy_analysis_from_labels_column.py
index 21c852c..0fc4371 100644
--- a/notebooks_as_py/accuracy_analysis_from_labels_column.py
+++ b/notebooks_as_py/accuracy_analysis_from_labels_column.py
@@ -1,64 +1,80 @@
-from splink.datasets import splink_datasets
-import altair as alt
-alt.renderers.enable("html")
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink

-df = splink_datasets.fake_1000
+from splink import splink_datasets

+df = splink_datasets.fake_1000
 df.head(2)

-from splink.duckdb.linker import DuckDBLinker
-from splink.duckdb.blocking_rule_library import block_on
-import splink.duckdb.comparison_template_library as ctl
-import splink.duckdb.comparison_library as cl
+from splink import SettingsCreator, Linker, block_on, DuckDBAPI

-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
+import splink.comparison_library as cl
+
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
         block_on("first_name"),
         block_on("surname"),
+        block_on("dob"),
+        block_on("email"),
     ],
-    "comparisons": [
-        ctl.name_comparison("first_name"),
-        ctl.name_comparison("surname"),
-        ctl.date_comparison("dob", cast_strings_to_date=True),
-        cl.exact_match("city", term_frequency_adjustments=True),
-        ctl.email_comparison("email", include_username_fuzzy_level=False),
+    comparisons=[
+        cl.ForenameSurnameComparison("first_name", "surname"),
+        cl.DateOfBirthComparison(
+            "dob",
+            input_is_string=True,
+        ),
+        cl.ExactMatch("city").configure(term_frequency_adjustments=True),
+        cl.EmailComparison("email"),
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)

-linker = DuckDBLinker(df, settings, set_up_basic_logging=False)
+db_api = DuckDBAPI()
+linker = Linker(df, settings, db_api=db_api)
 deterministic_rules = [
     "l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1",
     "l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1",
     "l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2",
-    "l.email = r.email"
+    "l.email = r.email",
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)
-
+linker.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.7
+)

-linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=5)
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6, seed=5)

-session_dob = linker.estimate_parameters_using_expectation_maximisation(block_on("dob"))
-session_email = linker.estimate_parameters_using_expectation_maximisation(block_on("email"))
+session_dob = linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("dob"), estimate_without_term_frequencies=True
+)
+session_email = linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("email"), estimate_without_term_frequencies=True
+)
+session_dob = linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("first_name", "surname"), estimate_without_term_frequencies=True
+)

-linker.truth_space_table_from_labels_column(
-    "cluster", match_weight_round_to_nearest=0.1
+linker.evaluation.accuracy_analysis_from_labels_column(
+    "cluster", output_type="table"
 ).as_pandas_dataframe(limit=5)

-linker.roc_chart_from_labels_column("cluster")
+linker.evaluation.accuracy_analysis_from_labels_column("cluster", output_type="roc")

-linker.precision_recall_chart_from_labels_column("cluster")
+linker.evaluation.accuracy_analysis_from_labels_column(
+    "cluster",
+    output_type="threshold_selection",
+    threshold_match_probability=0.5,
+    add_metrics=["f1"],
+)

 # Plot some false positives
-linker.prediction_errors_from_labels_column(
+linker.evaluation.prediction_errors_from_labels_column(
     "cluster", include_false_negatives=True, include_false_positives=True
 ).as_pandas_dataframe(limit=5)

-records = linker.prediction_errors_from_labels_column(
+records = linker.evaluation.prediction_errors_from_labels_column(
     "cluster", include_false_negatives=True, include_false_positives=True
 ).as_record_dict(limit=5)

-linker.waterfall_chart(records)
\ No newline at end of file
+linker.visualisations.waterfall_chart(records)
\ No newline at end of file
diff --git a/notebooks_as_py/deduplicate_1k_synthetic.py b/notebooks_as_py/deduplicate_1k_synthetic.py
index a8cb5fb..36d4657 100644
--- a/notebooks_as_py/deduplicate_1k_synthetic.py
+++ b/notebooks_as_py/deduplicate_1k_synthetic.py
@@ -1,13 +1,17 @@
-from splink.spark.jar_location import similarity_jar_location
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+# !pip install pyspark

-from pyspark import SparkContext, SparkConf
+from pyspark import SparkConf, SparkContext
 from pyspark.sql import SparkSession
-from pyspark.sql import types
+
+from splink.backends.spark import similarity_jar_location

 conf = SparkConf()
 # This parallelism setting is only suitable for a small toy example
 conf.set("spark.driver.memory", "12g")
-conf.set("spark.default.parallelism", "16")
+conf.set("spark.default.parallelism", "8")
+conf.set("spark.sql.codegen.wholeStage", "false")


 # Add custom similarity functions, which are bundled with Splink
@@ -22,56 +26,60 @@

 # Disable warnings for pyspark - you don't need to include this
 import warnings
+
 spark.sparkContext.setLogLevel("ERROR")
 warnings.simplefilter("ignore", UserWarning)

-from splink.datasets import splink_datasets
+from splink import splink_datasets
+
 pandas_df = splink_datasets.fake_1000

 df = spark.createDataFrame(pandas_df)

-import splink.spark.comparison_library as cl
-import splink.spark.comparison_template_library as ctl
-from splink.spark.blocking_rule_library import block_on
-
-settings = {
-    "link_type": "dedupe_only",
-    "comparisons": [
-        ctl.name_comparison("first_name"),
-        ctl.name_comparison("surname"),
-        ctl.date_comparison("dob", cast_strings_to_date=True),
-        cl.exact_match("city", term_frequency_adjustments=True),
-        ctl.email_comparison("email", include_username_fuzzy_level=False),
+import splink.comparison_library as cl
+from splink import Linker, SettingsCreator, SparkAPI, block_on
+
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    comparisons=[
+        cl.NameComparison("first_name"),
+        cl.NameComparison("surname"),
+        cl.LevenshteinAtThresholds(
+            "dob"
+        ),
+        cl.ExactMatch("city").configure(term_frequency_adjustments=True),
+        cl.EmailComparison("email"),
     ],
-    "blocking_rules_to_generate_predictions": [
+    blocking_rules_to_generate_predictions=[
         block_on("first_name"),
         "l.surname = r.surname",  # alternatively, you can write BRs in their SQL form
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-    "em_convergence": 0.01
-}
+    retain_intermediate_calculation_columns=True,
+    em_convergence=0.01,
+)

-from splink.spark.linker import SparkLinker
-linker = SparkLinker(df, settings, spark=spark)
+linker = Linker(df, settings, db_api=SparkAPI(spark_session=spark))
 deterministic_rules = [
     "l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1",
     "l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1",
     "l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2",
-    "l.email = r.email"
+    "l.email = r.email",
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)
-
+linker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)

-linker.estimate_u_using_random_sampling(max_pairs=5e5)
+linker.training.estimate_u_using_random_sampling(max_pairs=5e5)

 training_blocking_rule = "l.first_name = r.first_name and l.surname = r.surname"
-training_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+training_session_fname_sname = (
+    linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+)

 training_blocking_rule = "l.dob = r.dob"
-training_session_dob = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+training_session_dob = linker.training.estimate_parameters_using_expectation_maximisation(
+    training_blocking_rule
+)

-results = linker.predict(threshold_match_probability=0.9)
+results = linker.inference.predict(threshold_match_probability=0.9)

-results.as_pandas_dataframe(limit=5)
\ No newline at end of file
+spark_df = results.as_spark_dataframe().show()
\ No newline at end of file
diff --git a/notebooks_as_py/deduplicate_50k_synthetic.py b/notebooks_as_py/deduplicate_50k_synthetic.py
index 682bac4..84ba1e1 100644
--- a/notebooks_as_py/deduplicate_50k_synthetic.py
+++ b/notebooks_as_py/deduplicate_50k_synthetic.py
@@ -1,162 +1,135 @@
-from splink.athena.athena_linker import AthenaLinker
-import altair as alt
-alt.renderers.enable('mimetype')
-
-import pandas as pd
-pd.options.display.max_rows = 1000
-df = pd.read_parquet("./data/historical_figures_with_errors_50k.parquet")
-
 import boto3
-my_session = boto3.Session(region_name="eu-west-1")
-
-# Simple settings dictionary will be used for exploratory analysis
-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
-        "l.first_name = r.first_name and l.surname = r.surname",
-        "l.surname = r.surname and l.dob = r.dob",
-        "l.first_name = r.first_name and l.dob = r.dob",
-        "l.postcode_fake = r.postcode_fake and l.first_name = r.first_name",
-    ],
-}
+
+boto3_session = boto3.Session(region_name="eu-west-1")

 # Set the output bucket and the additional filepath to write outputs to
 ############################################
 # EDIT THESE BEFORE ATTEMPTING TO RUN THIS #
 ############################################

-bucket = "my_s3_bucket"
-database = "my_athena_database"
-filepath = "athena_testing"  # file path inside of your bucket
-aws_filepath = f"s3://{bucket}/{filepath}"
+from splink.backends.athena import AthenaAPI
+

-# Sessions are generated with a unique ID...
-linker = AthenaLinker(
-    input_table_or_tables=df,
-    boto3_session=my_session,
-    # the bucket to store splink's parquet files
+bucket = "MYTESTBUCKET"
+database = "MYTESTDATABASE"
+filepath = "MYTESTFILEPATH"  # file path inside of your bucket
+
+aws_filepath = f"s3://{bucket}/{filepath}"
+db_api = AthenaAPI(
+    boto3_session,
     output_bucket=bucket,
-    # the database to store splink's outputs
     output_database=database,
-    # folder to output data to
-    output_filepath=filepath,
-    # table name within your database
-    # if blank, it will default to __splink__input_table_randomid
-    input_table_aliases="__splink__testings",
-    settings_dict=settings,
+    output_filepath=filepath,
+)
+
+import splink.comparison_library as cl
+from splink import block_on
+
+from splink import Linker, SettingsCreator, splink_datasets
+
+df = splink_datasets.historical_50k
+
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
+        block_on("first_name", "surname"),
+        block_on("surname", "dob"),
+    ],
+    comparisons=[
+        cl.ExactMatch("first_name").configure(term_frequency_adjustments=True),
+        cl.LevenshteinAtThresholds("surname", [1, 3]),
+        cl.LevenshteinAtThresholds("dob", [1, 2]),
+        cl.LevenshteinAtThresholds("postcode_fake", [1, 2]),
+        cl.ExactMatch("birth_place").configure(term_frequency_adjustments=True),
+        cl.ExactMatch("occupation").configure(term_frequency_adjustments=True),
+    ],
+    retain_intermediate_calculation_columns=True,
 )

-linker.profile_columns(
-    ["first_name", "postcode_fake", "substr(dob, 1,4)"], top_n=10, bottom_n=5
+from splink.exploratory import profile_columns
+
+profile_columns(df, db_api, column_expressions=["first_name", "substr(surname,1,2)"])
+
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)
+from splink import block_on
+
+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=df,
+    db_api=db_api,
+    blocking_rules=[block_on("first_name", "surname"), block_on("surname", "dob")],
+    link_type="dedupe_only",
 )

-linker.cumulative_num_comparisons_from_blocking_rules_chart()
+import splink.comparison_library as cl

-linker.drop_all_tables_created_by_splink(delete_s3_folders=True)

-import splink.athena.athena_comparison_library as cl
+from splink import Linker, SettingsCreator

-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
-        "l.first_name = r.first_name and l.surname = r.surname",
-        "l.surname = r.surname and l.dob = r.dob",
-        "l.first_name = r.first_name and l.dob = r.dob",
-        "l.postcode_fake = r.postcode_fake and l.first_name = r.first_name",
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
+        block_on("first_name", "surname"),
+        block_on("surname", "dob"),
     ],
-    "comparisons": [
-        cl.levenshtein_at_thresholds("first_name", [1,2], term_frequency_adjustments=True),
-        cl.levenshtein_at_thresholds("surname", [1,2], term_frequency_adjustments=True),
-        cl.levenshtein_at_thresholds("dob", [1,2], term_frequency_adjustments=True),
-        cl.levenshtein_at_thresholds("postcode_fake", 2,term_frequency_adjustments=True),
-        cl.exact_match("birth_place", term_frequency_adjustments=True),
-        cl.exact_match("occupation",  term_frequency_adjustments=True),
+    comparisons=[
+        cl.ExactMatch("first_name").configure(term_frequency_adjustments=True),
+        cl.LevenshteinAtThresholds("surname", [1, 3]),
+        cl.LevenshteinAtThresholds("dob", [1, 2]),
+        cl.LevenshteinAtThresholds("postcode_fake", [1, 2]),
+        cl.ExactMatch("birth_place").configure(term_frequency_adjustments=True),
+        cl.ExactMatch("occupation").configure(term_frequency_adjustments=True),
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-    "max_iterations": 10,
-    "em_convergence": 0.01
-}
-
-# Write our dataframe to s3/our backing database
-import awswrangler as wr
-wr.s3.to_parquet(
-    df,  # pandas dataframe
-    path=f"{aws_filepath}/historical_figures_with_errors_50k",
-    dataset=True,
-    database=database,
-    table="historical_figures_with_errors_50k",
-    mode="overwrite",
-    compression="snappy",
+    retain_intermediate_calculation_columns=True,
 )

-# Initialise our linker with historical_figures_with_errors_50k from our database
-linker = AthenaLinker(
-    input_table_or_tables="historical_figures_with_errors_50k",
-    settings_dict=settings,
-    boto3_session=my_session,
-    output_bucket=bucket,  # the bucket to store splink's parquet files
-    output_database=database,  # the database to store splink's outputs
-    output_filepath=filepath  # folder to output data to
-)
+linker = Linker(df, settings, db_api=db_api)

-linker.estimate_probability_two_random_records_match(
+linker.training.estimate_probability_two_random_records_match(
     [
-        "l.first_name = r.first_name and l.surname = r.surname and l.dob = r.dob",
-        "substr(l.first_name,1,2) = substr(r.first_name,1,2) and l.surname = r.surname and substr(l.postcode_fake,1,2) = substr(r.postcode_fake,1,2)",
-        "l.dob = r.dob and l.postcode_fake = r.postcode_fake",
+        block_on("first_name", "surname", "dob"),
+        block_on("substr(first_name,1,2)", "surname", "substr(postcode_fake, 1,2)"),
+        block_on("dob", "postcode_fake"),
     ],
     recall=0.6,
 )

-linker.estimate_u_using_random_sampling(max_pairs=5e6)
+linker.training.estimate_u_using_random_sampling(max_pairs=5e6)

-blocking_rule = "l.first_name = r.first_name and l.surname = r.surname"
-training_session_names = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)
+blocking_rule = block_on("first_name", "surname")
+training_session_names = (
+    linker.training.estimate_parameters_using_expectation_maximisation(blocking_rule)
+)

-blocking_rule = "l.dob = r.dob"
-training_session_dob = linker.estimate_parameters_using_expectation_maximisation(blocking_rule)
+blocking_rule = block_on("dob")
+training_session_dob = (
+    linker.training.estimate_parameters_using_expectation_maximisation(blocking_rule)
+)

-linker.match_weights_chart()
+linker.visualisations.match_weights_chart()

-linker.unlinkables_chart()
+linker.evaluation.unlinkables_chart()

-df_predict = linker.predict()
+df_predict = linker.inference.predict()
 df_e = df_predict.as_pandas_dataframe(limit=5)
 df_e

-from splink.charts import waterfall_chart
 records_to_plot = df_e.to_dict(orient="records")
-linker.waterfall_chart(records_to_plot, filter_nulls=False)
-
-clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=0.95)
+linker.visualisations.waterfall_chart(records_to_plot, filter_nulls=False)

-linker.cluster_studio_dashboard(df_predict, clusters, "dashboards/50k_cluster.html", sampling_method='by_cluster_size', overwrite=True)
-
-from IPython.display import IFrame
-
-IFrame(
-    src="./dashboards/50k_cluster.html", width="100%", height=1200
+clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(
+    df_predict, threshold_match_probability=0.95
 )

-linker.roc_chart_from_labels_column("cluster",match_weight_round_to_nearest=0.02)
-
-records = linker.prediction_errors_from_labels_column(
-    "cluster",
-    threshold=0.999,
-    include_false_negatives=False,
-    include_false_positives=True,
-).as_record_dict()
-linker.waterfall_chart(records)
-
-# Some of the false negatives will be because they weren't detected by the blocking rules
-records = linker.prediction_errors_from_labels_column(
-    "cluster",
-    threshold=0.5,
-    include_false_negatives=True,
-    include_false_positives=False,
-).as_record_dict(limit=50)
+linker.visualisations.cluster_studio_dashboard(
+    df_predict,
+    clusters,
+    "dashboards/50k_cluster.html",
+    sampling_method="by_cluster_size",
+    overwrite=True,
+)

-linker.waterfall_chart(records)
+from IPython.display import IFrame

-linker.drop_tables_in_current_splink_run(tables_to_exclude=df_predict)
\ No newline at end of file
+IFrame(src="./dashboards/50k_cluster.html", width="100%", height=1200)
\ No newline at end of file
diff --git a/notebooks_as_py/deterministic_dedupe.py b/notebooks_as_py/deterministic_dedupe.py
index 794fb8a..233a5d2 100644
--- a/notebooks_as_py/deterministic_dedupe.py
+++ b/notebooks_as_py/deterministic_dedupe.py
@@ -1,47 +1,63 @@
-from splink.datasets import splink_datasets
-from splink.duckdb.linker import DuckDBLinker
-import altair as alt
-alt.renderers.enable('html')
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+import pandas as pd
+
+from splink import splink_datasets

-import pandas as pd
 pd.options.display.max_rows = 1000
 df = splink_datasets.historical_50k
 df.head()

-from splink.duckdb.blocking_rule_library import block_on
+from splink import DuckDBAPI, block_on
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)

-# Simple settings dictionary will be used for exploratory analysis
-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
-        block_on(["first_name", "surname", "dob"]),
-        block_on(["surname", "dob", "postcode_fake"]),
-        block_on(["first_name", "dob", "occupation"]),
+db_api = DuckDBAPI()
+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=df,
+    blocking_rules=[
+        block_on("first_name", "surname", "dob"),
+        block_on("surname", "dob", "postcode_fake"),
+        block_on("first_name", "dob", "occupation"),
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-}
-linker = DuckDBLinker(df, settings)
+    db_api=db_api,
+    link_type="dedupe_only",
+)

-linker.debug_mode = False
+from splink import Linker, SettingsCreator

-linker.profile_columns(
-    ["first_name", "surname", "substr(dob, 1,4)"], top_n=10, bottom_n=5
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
+        block_on("first_name", "surname", "dob"),
+        block_on("surname", "dob", "postcode_fake"),
+        block_on("first_name", "dob", "occupation"),
+    ],
+    retain_intermediate_calculation_columns=True,
 )

-linker.cumulative_num_comparisons_from_blocking_rules_chart()
+linker = Linker(df, settings, db_api=db_api)
+

-df_predict = linker.deterministic_link()
+df_predict = linker.inference.deterministic_link()
 df_predict.as_pandas_dataframe().head()

-clusters = linker.cluster_pairwise_predictions_at_threshold(df_predict, threshold_match_probability=1)
+clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(
+    df_predict, threshold_match_probability=1
+)

 clusters.as_pandas_dataframe(limit=5)

-linker.cluster_studio_dashboard(df_predict, clusters, "dashboards/50k_deterministic_cluster.html", sampling_method='by_cluster_size', overwrite=True)
+linker.visualisations.cluster_studio_dashboard(
+    df_predict,
+    clusters,
+    "dashboards/50k_deterministic_cluster.html",
+    sampling_method="by_cluster_size",
+    overwrite=True,
+)

 from IPython.display import IFrame

-IFrame(
-    src="./dashboards/50k_deterministic_cluster.html", width="100%", height=1200
-)
\ No newline at end of file
+IFrame(src="./dashboards/50k_deterministic_cluster.html", width="100%", height=1200)
\ No newline at end of file
diff --git a/notebooks_as_py/febrl3.py b/notebooks_as_py/febrl3.py
index 943c8f4..29ef771 100644
--- a/notebooks_as_py/febrl3.py
+++ b/notebooks_as_py/febrl3.py
@@ -1,98 +1,129 @@
-import pandas as pd
-import altair as alt
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
 from splink.datasets import splink_datasets

 df = splink_datasets.febrl3
+
 df = df.rename(columns=lambda x: x.strip())

-df["cluster"] = df["rec_id"].apply(lambda x: "-".join(x.split('-')[:2]))
+df["cluster"] = df["rec_id"].apply(lambda x: "-".join(x.split("-")[:2]))

-# dob and ssn needs to be a string for fuzzy comparisons like levenshtein to be applied
 df["date_of_birth"] = df["date_of_birth"].astype(str).str.strip()
-df["date_of_birth"] = df["date_of_birth"].replace("", None)
-
 df["soc_sec_id"] = df["soc_sec_id"].astype(str).str.strip()
-df["soc_sec_id"] = df["soc_sec_id"].replace("", None)

-df["postcode"] = df["postcode"].astype(str).str.strip()
-df["postcode"] = df["postcode"].replace("", None)
 df.head(2)

+df["date_of_birth"] = df["date_of_birth"].astype(str).str.strip()
+df["soc_sec_id"] = df["soc_sec_id"].astype(str).str.strip()

-from splink.duckdb.linker import DuckDBLinker
+df["date_of_birth"] = df["date_of_birth"].astype(str).str.strip()
+df["soc_sec_id"] = df["soc_sec_id"].astype(str).str.strip()

-settings = {
-    "unique_id_column_name": "rec_id",
-    "link_type": "dedupe_only",
-}
+from splink import DuckDBAPI, Linker, SettingsCreator

-linker = DuckDBLinker(df, settings)
+# TODO:  Allow missingness to be analysed without a linker
+settings = SettingsCreator(
+    unique_id_column_name="rec_id",
+    link_type="dedupe_only",
+)

-linker.missingness_chart()
+linker = Linker(df, settings, db_api=DuckDBAPI())

-linker.profile_columns(list(df.columns))
+from splink.exploratory import completeness_chart

-from splink.duckdb.blocking_rule_library import block_on
+completeness_chart(df, db_api=DuckDBAPI())
+
+from splink.exploratory import profile_columns
+
+profile_columns(df, db_api=DuckDBAPI(), column_expressions=["given_name", "surname"])
+
+from splink import DuckDBAPI, block_on
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)

 blocking_rules = [
-        block_on("soc_sec_id"),
-        block_on("given_name"),
-        block_on("surname"),
-        block_on("date_of_birth"),
-        block_on("postcode"),
+    block_on("soc_sec_id"),
+    block_on("given_name"),
+    block_on("surname"),
+    block_on("date_of_birth"),
+    block_on("postcode"),
 ]
-linker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)
-
-from splink.duckdb.linker import DuckDBLinker
-import splink.duckdb.comparison_library as cl
-import splink.duckdb.comparison_template_library as ctl
-
-
-settings = {
-    "unique_id_column_name": "rec_id",
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": blocking_rules,
-    "comparisons": [
-        ctl.name_comparison("given_name", term_frequency_adjustments=True),
-        ctl.name_comparison("surname", term_frequency_adjustments=True),
-        ctl.date_comparison("date_of_birth",
-                            damerau_levenshtein_thresholds=[],
-                            cast_strings_to_date=True,
-                            invalid_dates_as_null=True,
-                            date_format="%Y%m%d"),
-        cl.levenshtein_at_thresholds("soc_sec_id", [2]),
-        cl.exact_match("street_number", term_frequency_adjustments=True),
-        cl.exact_match("postcode", term_frequency_adjustments=True),
+
+db_api = DuckDBAPI()
+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=df,
+    blocking_rules=blocking_rules,
+    db_api=db_api,
+    link_type="dedupe_only",
+    unique_id_column_name="rec_id",
+)
+
+import splink.comparison_library as cl
+
+from splink import Linker
+
+settings = SettingsCreator(
+    unique_id_column_name="rec_id",
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=blocking_rules,
+    comparisons=[
+        cl.NameComparison("given_name"),
+        cl.NameComparison("surname"),
+        cl.DateOfBirthComparison(
+            "date_of_birth",
+            input_is_string=True,
+            datetime_format="%Y%m%d",
+        ),
+        cl.DamerauLevenshteinAtThresholds("soc_sec_id", [2]),
+        cl.ExactMatch("street_number").configure(term_frequency_adjustments=True),
+        cl.ExactMatch("postcode").configure(term_frequency_adjustments=True),
     ],
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)
+
+linker = Linker(df, settings, db_api=DuckDBAPI())

-linker = DuckDBLinker(df, settings)
+from splink import block_on

 deterministic_rules = [
-    "l.soc_sec_id = r.soc_sec_id",
-    "l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth",
-    "l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth"
+    block_on("soc_sec_id"),
+    block_on("given_name", "surname", "date_of_birth"),
+    "l.given_name = r.surname and l.surname = r.given_name and l.date_of_birth = r.date_of_birth",
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.9)
+linker.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.9
+)
+
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6)

-linker.estimate_u_using_random_sampling(max_pairs=1e6)
+em_blocking_rule_1 = block_on("date_of_birth")
+session_dob = linker.training.estimate_parameters_using_expectation_maximisation(
+    em_blocking_rule_1
+)

-em_blocking_rule_1 = block_on("substr(date_of_birth,1,3)")
-em_blocking_rule_2 = block_on("substr(postcode,1,2)")
-session_dob = linker.estimate_parameters_using_expectation_maximisation(em_blocking_rule_1)
-session_postcode = linker.estimate_parameters_using_expectation_maximisation(em_blocking_rule_2)
+em_blocking_rule_2 = block_on("postcode")
+session_postcode = linker.training.estimate_parameters_using_expectation_maximisation(
+    em_blocking_rule_2
+)

-linker.match_weights_chart()
+linker.visualisations.match_weights_chart()

-results = linker.predict(threshold_match_probability=0.2)
+results = linker.inference.predict(threshold_match_probability=0.2)

-linker.roc_chart_from_labels_column("cluster")
+linker.evaluation.accuracy_analysis_from_labels_column(
+    "cluster", match_weight_round_to_nearest=0.1, output_type="accuracy"
+)

-pred_errors_df = linker.prediction_errors_from_labels_column("cluster").as_pandas_dataframe()
+pred_errors_df = linker.evaluation.prediction_errors_from_labels_column(
+    "cluster"
+).as_pandas_dataframe()
 len(pred_errors_df)
 pred_errors_df.head()

-records = linker.prediction_errors_from_labels_column("cluster").as_record_dict(limit=10)
-linker.waterfall_chart(records)
\ No newline at end of file
+records = linker.evaluation.prediction_errors_from_labels_column(
+    "cluster"
+).as_record_dict(limit=10)
+linker.visualisations.waterfall_chart(records)
\ No newline at end of file
diff --git a/notebooks_as_py/febrl4.py b/notebooks_as_py/febrl4.py
index c4d541a..0464dab 100644
--- a/notebooks_as_py/febrl4.py
+++ b/notebooks_as_py/febrl4.py
@@ -1,131 +1,162 @@
-import pandas as pd
-import altair as alt
-from splink.datasets import splink_datasets
-from IPython.display import IFrame
-alt.renderers.enable('html')
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import splink_datasets

 df_a = splink_datasets.febrl4a
 df_b = splink_datasets.febrl4b

+
 def prepare_data(data):
     data = data.rename(columns=lambda x: x.strip())
-    data["cluster"] = data["rec_id"].apply(lambda x: "-".join(x.split('-')[:2]))
+    data["cluster"] = data["rec_id"].apply(lambda x: "-".join(x.split("-")[:2]))
     data["date_of_birth"] = data["date_of_birth"].astype(str).str.strip()
-    data["date_of_birth"] = data["date_of_birth"].replace("", None)
-
     data["soc_sec_id"] = data["soc_sec_id"].astype(str).str.strip()
-    data["soc_sec_id"] = data["soc_sec_id"].replace("", None)
-
     data["postcode"] = data["postcode"].astype(str).str.strip()
-    data["postcode"] = data["postcode"].replace("", None)
     return data
-dfs = [
-    prepare_data(dataset)
-    for dataset in [df_a, df_b]
-]
+
+
+dfs = [prepare_data(dataset) for dataset in [df_a, df_b]]

 display(dfs[0].head(2))
 display(dfs[1].head(2))

-from splink.duckdb.linker import DuckDBLinker
+from splink import DuckDBAPI, Linker, SettingsCreator

-basic_settings = {
-    "unique_id_column_name": "rec_id",
-    "link_type": "link_only",
+basic_settings = SettingsCreator(
+    unique_id_column_name="rec_id",
+    link_type="link_only",
     # NB as we are linking one-one, we know the probability that a random pair will be a match
     # hence we could set:
     # "probability_two_random_records_match": 1/5000,
     # however we will not specify this here, as we will use this as a check that
     # our estimation procedure returns something sensible
-}
+)

-linker = DuckDBLinker(dfs, basic_settings)
+linker = Linker(dfs, basic_settings, db_api=DuckDBAPI())

-linker.missingness_chart()
+from splink.exploratory import completeness_chart

-cols_to_profile = list(dfs[0].columns)
-cols_to_profile = [col for col in cols_to_profile if col not in ("rec_id", "cluster")]
-linker.profile_columns(cols_to_profile)
+completeness_chart(dfs, db_api=DuckDBAPI())
+
+from splink.exploratory import profile_columns
+
+profile_columns(dfs, db_api=DuckDBAPI(), column_expressions=["given_name", "surname"])
+
+from splink import DuckDBAPI, block_on
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)

 blocking_rules = [
-    "l.given_name = r.given_name AND l.surname = r.surname",
-    "l.date_of_birth = r.date_of_birth",
-    "l.soc_sec_id = r.soc_sec_id",
-    "l.state = r.state AND l.address_1 = r.address_1",
-    "l.street_number = r.street_number AND l.address_1 = r.address_1",
-    "l.postcode = r.postcode",
+    block_on("given_name", "surname"),
+    # A blocking rule can also be an aribtrary SQL expression
+    "l.given_name = r.surname and l.surname = r.given_name",
+    block_on("date_of_birth"),
+    block_on("soc_sec_id"),
+    block_on("state", "address_1"),
+    block_on("street_number", "address_1"),
+    block_on("postcode"),
 ]
-linker.cumulative_num_comparisons_from_blocking_rules_chart(blocking_rules)

-import splink.duckdb.comparison_library as cl
-import splink.duckdb.comparison_template_library as ctl
-import splink.duckdb.comparison_level_library as cll
+
+db_api = DuckDBAPI()
+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=dfs,
+    blocking_rules=blocking_rules,
+    db_api=db_api,
+    link_type="link_only",
+    unique_id_column_name="rec_id",
+    source_dataset_column_name="source_dataset",
+)
+
+import splink.comparison_level_library as cll
+import splink.comparison_library as cl
+

 # the simple model only considers a few columns, and only two comparison levels for each
-simple_model_settings = {
-    **basic_settings,
-    "blocking_rules_to_generate_predictions": blocking_rules,
-    "comparisons": [
-        cl.exact_match("given_name", term_frequency_adjustments=True),
-        cl.exact_match("surname", term_frequency_adjustments=True),
-        cl.exact_match("street_number", term_frequency_adjustments=True),
+simple_model_settings = SettingsCreator(
+    unique_id_column_name="rec_id",
+    link_type="link_only",
+    blocking_rules_to_generate_predictions=blocking_rules,
+    comparisons=[
+        cl.ExactMatch("given_name").configure(term_frequency_adjustments=True),
+        cl.ExactMatch("surname").configure(term_frequency_adjustments=True),
+        cl.ExactMatch("street_number").configure(term_frequency_adjustments=True),
     ],
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)
+
 # the detailed model considers more columns, using the information we saw in the exploratory phase
 # we also include further comparison levels to account for typos and other differences
-detailed_model_settings = {
-    **basic_settings,
-    "blocking_rules_to_generate_predictions": blocking_rules,
-    "comparisons": [
-        ctl.name_comparison("given_name", term_frequency_adjustments=True),
-        ctl.name_comparison("surname", term_frequency_adjustments=True),
-        ctl.date_comparison("date_of_birth",
-                            damerau_levenshtein_thresholds=[],
-                            cast_strings_to_date=True,
-                            invalid_dates_as_null=True,
-                            date_format="%Y%m%d"),
-        cl.damerau_levenshtein_at_thresholds("soc_sec_id", [1, 2]),
-        cl.exact_match("street_number", term_frequency_adjustments=True),
-        cl.damerau_levenshtein_at_thresholds("postcode", [1, 2], term_frequency_adjustments=True),
+detailed_model_settings = SettingsCreator(
+    unique_id_column_name="rec_id",
+    link_type="link_only",
+    blocking_rules_to_generate_predictions=blocking_rules,
+    comparisons=[
+        cl.NameComparison("given_name").configure(term_frequency_adjustments=True),
+        cl.NameComparison("surname").configure(term_frequency_adjustments=True),
+        cl.DateOfBirthComparison(
+            "date_of_birth",
+            input_is_string=True,
+            datetime_format="%Y%m%d",
+            invalid_dates_as_null=True,
+        ),
+        cl.DamerauLevenshteinAtThresholds("soc_sec_id", [1, 2]),
+        cl.ExactMatch("street_number").configure(term_frequency_adjustments=True),
+        cl.DamerauLevenshteinAtThresholds("postcode", [1, 2]).configure(
+            term_frequency_adjustments=True
+        ),
         # we don't consider further location columns as they will be strongly correlated with postcode
     ],
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)


-linker_simple = DuckDBLinker(dfs, simple_model_settings)
-linker_detailed = DuckDBLinker(dfs, detailed_model_settings)
+linker_simple = Linker(dfs, simple_model_settings, db_api=DuckDBAPI())
+linker_detailed = Linker(dfs, detailed_model_settings, db_api=DuckDBAPI())

 deterministic_rules = [
-    "l.soc_sec_id = r.soc_sec_id",
-    "l.given_name = r.given_name and l.surname = r.surname and l.date_of_birth = r.date_of_birth",
+    block_on("soc_sec_id"),
+    block_on("given_name", "surname", "date_of_birth"),
 ]

-linker_detailed.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)
+linker_detailed.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.8
+)

-linker_detailed.estimate_u_using_random_sampling(max_pairs=1e7)
+# We generally recommend setting max pairs higher (e.g. 1e7 or more)
+# But this will run faster for the purpose of this demo
+linker_detailed.training.estimate_u_using_random_sampling(max_pairs=1e6)

-session_dob = linker_detailed.estimate_parameters_using_expectation_maximisation(
-    "l.date_of_birth = r.date_of_birth"
+session_dob = (
+    linker_detailed.training.estimate_parameters_using_expectation_maximisation(
+        block_on("date_of_birth"), estimate_without_term_frequencies=True
+    )
 )
-session_pc = linker_detailed.estimate_parameters_using_expectation_maximisation(
-    "l.postcode = r.postcode"
+session_pc = (
+    linker_detailed.training.estimate_parameters_using_expectation_maximisation(
+        block_on("postcode"), estimate_without_term_frequencies=True
+    )
 )

 session_dob.m_u_values_interactive_history_chart()

-linker_detailed.parameter_estimate_comparisons_chart()
+linker_detailed.visualisations.parameter_estimate_comparisons_chart()

-linker_simple.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)
-linker_simple.estimate_u_using_random_sampling(max_pairs=1e7)
-session_ssid = linker_simple.estimate_parameters_using_expectation_maximisation(
-    "l.given_name = r.given_name"
+linker_simple.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.8
 )
-session_pc = linker_simple.estimate_parameters_using_expectation_maximisation(
-    "l.street_number = r.street_number"
+linker_simple.training.estimate_u_using_random_sampling(max_pairs=1e7)
+session_ssid = (
+    linker_simple.training.estimate_parameters_using_expectation_maximisation(
+        block_on("given_name"), estimate_without_term_frequencies=True
+    )
 )
-linker_simple.parameter_estimate_comparisons_chart()
+session_pc = linker_simple.training.estimate_parameters_using_expectation_maximisation(
+    block_on("street_number"), estimate_without_term_frequencies=True
+)
+linker_simple.visualisations.parameter_estimate_comparisons_chart()

 # import json
 # we can have a look at the full settings if we wish, including the values of our estimated parameters:
@@ -134,37 +165,52 @@ def prepare_data(data):
 # print(linker_detailed._settings_obj.human_readable_description)
 # (we suppress output here for brevity)

-linker_simple.match_weights_chart()
+linker_simple.visualisations.match_weights_chart()

-linker_detailed.match_weights_chart()
+linker_detailed.visualisations.match_weights_chart()

 # linker_simple.m_u_parameters_chart()
-linker_detailed.m_u_parameters_chart()
+linker_detailed.visualisations.m_u_parameters_chart()

-linker_simple.unlinkables_chart()
+linker_simple.evaluation.unlinkables_chart()

-linker_detailed.unlinkables_chart()
+linker_detailed.evaluation.unlinkables_chart()

-predictions = linker_detailed.predict()
+predictions = linker_detailed.inference.predict(threshold_match_probability=0.2)
 df_predictions = predictions.as_pandas_dataframe()
 df_predictions.head(5)

-# linker_detailed.roc_chart_from_labels_column("cluster")
-linker_detailed.precision_recall_chart_from_labels_column("cluster")
+linker_detailed.evaluation.accuracy_analysis_from_labels_column(
+    "cluster", output_type="accuracy"
+)

-clusters = linker_detailed.cluster_pairwise_predictions_at_threshold(predictions, threshold_match_probability=0.99)
+clusters = linker_detailed.clustering.cluster_pairwise_predictions_at_threshold(
+    predictions, threshold_match_probability=0.99
+)
 df_clusters = clusters.as_pandas_dataframe().sort_values("cluster_id")
 df_clusters.groupby("cluster_id").size().value_counts()

-df_predictions["cluster_l"] = df_predictions["rec_id_l"].apply(lambda x: "-".join(x.split('-')[:2]))
-df_predictions["cluster_r"] = df_predictions["rec_id_r"].apply(lambda x: "-".join(x.split('-')[:2]))
-df_true_links = df_predictions[df_predictions["cluster_l"] == df_predictions["cluster_r"]].sort_values("match_probability")
+df_predictions["cluster_l"] = df_predictions["rec_id_l"].apply(
+    lambda x: "-".join(x.split("-")[:2])
+)
+df_predictions["cluster_r"] = df_predictions["rec_id_r"].apply(
+    lambda x: "-".join(x.split("-")[:2])
+)
+df_true_links = df_predictions[
+    df_predictions["cluster_l"] == df_predictions["cluster_r"]
+].sort_values("match_probability")

 records_to_view = 3
-linker_detailed.waterfall_chart(df_true_links.head(records_to_view).to_dict(orient="records"))
+linker_detailed.visualisations.waterfall_chart(
+    df_true_links.head(records_to_view).to_dict(orient="records")
+)

-df_non_links = df_predictions[df_predictions["cluster_l"] != df_predictions["cluster_r"]].sort_values("match_probability", ascending=False)
-linker_detailed.waterfall_chart(df_non_links.head(records_to_view).to_dict(orient="records"))
+df_non_links = df_predictions[
+    df_predictions["cluster_l"] != df_predictions["cluster_r"]
+].sort_values("match_probability", ascending=False)
+linker_detailed.visualisations.waterfall_chart(
+    df_non_links.head(records_to_view).to_dict(orient="records")
+)

 # we need to append a full name column to our source data frames
 # so that we can use it for term frequency adjustments
@@ -173,7 +219,8 @@ def prepare_data(data):


 extended_model_settings = {
-    **basic_settings,
+    "unique_id_column_name": "rec_id",
+    "link_type": "link_only",
     "blocking_rules_to_generate_predictions": blocking_rules,
     "comparisons": [
         {
@@ -185,68 +232,69 @@ def prepare_data(data):
                     "is_null_level": True,
                 },
                 # full name match
-                cll.exact_match_level("full_name", term_frequency_adjustments=True),
+                cll.ExactMatchLevel("full_name", term_frequency_adjustments=True),
                 # typos - keep levels across full name rather than scoring separately
-                cll.jaro_winkler_level("full_name", 0.9),
-                cll.jaro_winkler_level("full_name", 0.7),
+                cll.JaroWinklerLevel("full_name", 0.9),
+                cll.JaroWinklerLevel("full_name", 0.7),
                 # name switched
-                cll.columns_reversed_level("given_name", "surname"),
+                cll.ColumnsReversedLevel("given_name", "surname"),
                 # name switched + typo
                 {
                     "sql_condition": "jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.8",
-                    "label_for_charts": "switched + jaro_winkler_similarity >= 1.8"
+                    "label_for_charts": "switched + jaro_winkler_similarity >= 1.8",
                 },
                 {
                     "sql_condition": "jaro_winkler_similarity(given_name_l, surname_r) + jaro_winkler_similarity(surname_l, given_name_r) >= 1.4",
-                    "label_for_charts": "switched + jaro_winkler_similarity >= 1.4"
+                    "label_for_charts": "switched + jaro_winkler_similarity >= 1.4",
                 },
                 # single name match
-                cll.exact_match_level("given_name", term_frequency_adjustments=True),
-                cll.exact_match_level("surname", term_frequency_adjustments=True),
+                cll.ExactMatchLevel("given_name", term_frequency_adjustments=True),
+                cll.ExactMatchLevel("surname", term_frequency_adjustments=True),
                 # single name cross-match
                 {
                     "sql_condition": "given_name_l = surname_r OR surname_l = given_name_r",
-                    "label_for_charts": "single name cross-matches"
-                },                # single name typos
-                cll.jaro_winkler_level("given_name", 0.9),
-                cll.jaro_winkler_level("surname", 0.9),
+                    "label_for_charts": "single name cross-matches",
+                },  # single name typos
+                cll.JaroWinklerLevel("given_name", 0.9),
+                cll.JaroWinklerLevel("surname", 0.9),
                 # the rest
-                cll.else_level()
-            ]
+                cll.ElseLevel(),
+            ],
         },
-        ctl.date_comparison("date_of_birth",
-                            damerau_levenshtein_thresholds=[],
-                            cast_strings_to_date=True,
-                            invalid_dates_as_null=True,
-                            date_format="%Y%m%d"),
+        cl.DateOfBirthComparison(
+            "date_of_birth",
+            input_is_string=True,
+            datetime_format="%Y%m%d",
+            invalid_dates_as_null=True,
+        ),
         {
             "output_column_name": "Social security ID",
             "comparison_levels": [
-                cll.null_level("soc_sec_id"),
-                cll.exact_match_level("soc_sec_id", term_frequency_adjustments=True),
-                cll.damerau_levenshtein_level("soc_sec_id", 1),
-                cll.damerau_levenshtein_level("soc_sec_id", 2),
-                cll.else_level()
-            ]
+                cll.NullLevel("soc_sec_id"),
+                cll.ExactMatchLevel("soc_sec_id", term_frequency_adjustments=True),
+                cll.DamerauLevenshteinLevel("soc_sec_id", 1),
+                cll.DamerauLevenshteinLevel("soc_sec_id", 2),
+                cll.ElseLevel(),
+            ],
         },
         {
             "output_column_name": "Street number",
             "comparison_levels": [
-                cll.null_level("street_number"),
-                cll.exact_match_level("street_number", term_frequency_adjustments=True),
-                cll.damerau_levenshtein_level("street_number", 1),
-                cll.else_level()
-            ]
+                cll.NullLevel("street_number"),
+                cll.ExactMatchLevel("street_number", term_frequency_adjustments=True),
+                cll.DamerauLevenshteinLevel("street_number", 1),
+                cll.ElseLevel(),
+            ],
         },
         {
             "output_column_name": "Postcode",
             "comparison_levels": [
-                cll.null_level("postcode"),
-                cll.exact_match_level("postcode", term_frequency_adjustments=True),
-                cll.damerau_levenshtein_level("postcode", 1),
-                cll.damerau_levenshtein_level("postcode", 2),
-                cll.else_level()
-            ]
+                cll.NullLevel("postcode"),
+                cll.ExactMatchLevel("postcode", term_frequency_adjustments=True),
+                cll.DamerauLevenshteinLevel("postcode", 1),
+                cll.DamerauLevenshteinLevel("postcode", 2),
+                cll.ElseLevel(),
+            ],
         },
         # we don't consider further location columns as they will be strongly correlated with postcode
     ],
@@ -254,25 +302,36 @@ def prepare_data(data):
 }

 # train
-linker_advanced = DuckDBLinker(dfs, extended_model_settings)
-linker_advanced.estimate_probability_two_random_records_match(deterministic_rules, recall=0.8)
-# we increase target rows to improve accuracy for u values in full name comparison, as we have subdivided the data more finely
-linker_advanced.estimate_u_using_random_sampling(max_pairs=1e8)
-session_dob = linker_advanced.estimate_parameters_using_expectation_maximisation(
-    "l.date_of_birth = r.date_of_birth"
+linker_advanced = Linker(dfs, extended_model_settings, db_api=DuckDBAPI())
+linker_advanced.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.8
 )
+# We recommend increasing target rows to 1e8 improve accuracy for u
+# values in full name comparison, as we have subdivided the data more finely

+# Here, 1e7 for speed
+linker_advanced.training.estimate_u_using_random_sampling(max_pairs=1e7)

-session_pc = linker_advanced.estimate_parameters_using_expectation_maximisation(
-    "l.postcode = r.postcode"
+session_dob = (
+    linker_advanced.training.estimate_parameters_using_expectation_maximisation(
+        "l.date_of_birth = r.date_of_birth", estimate_without_term_frequencies=True
+    )
 )

-linker_advanced.parameter_estimate_comparisons_chart()
+session_pc = (
+    linker_advanced.training.estimate_parameters_using_expectation_maximisation(
+        "l.postcode = r.postcode", estimate_without_term_frequencies=True
+    )
+)

-linker_advanced.match_weights_chart()
+linker_advanced.visualisations.parameter_estimate_comparisons_chart()

-predictions_adv = linker_advanced.predict()
+linker_advanced.visualisations.match_weights_chart()
+
+predictions_adv = linker_advanced.inference.predict()
 df_predictions_adv = predictions_adv.as_pandas_dataframe()
-clusters_adv = linker_advanced.cluster_pairwise_predictions_at_threshold(predictions_adv, threshold_match_probability=0.99)
+clusters_adv = linker_advanced.clustering.cluster_pairwise_predictions_at_threshold(
+    predictions_adv, threshold_match_probability=0.99
+)
 df_clusters_adv = clusters_adv.as_pandas_dataframe().sort_values("cluster_id")
 df_clusters_adv.groupby("cluster_id").size().value_counts()
\ No newline at end of file
diff --git a/notebooks_as_py/link_only.py b/notebooks_as_py/link_only.py
index b5eb0fe..09eb612 100644
--- a/notebooks_as_py/link_only.py
+++ b/notebooks_as_py/link_only.py
@@ -1,4 +1,8 @@
-from splink.datasets import splink_datasets
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import splink_datasets
+
 df = splink_datasets.fake_1000

 # Split a simple dataset into two, separate datasets which can be linked together.
@@ -7,47 +11,68 @@

 df_l.head(2)

-from splink.duckdb.linker import DuckDBLinker
-from splink.duckdb.blocking_rule_library import block_on
-import splink.duckdb.comparison_library as cl
-import splink.duckdb.comparison_template_library as ctl
+import splink.comparison_library as cl

+from splink import DuckDBAPI, Linker, SettingsCreator, block_on

-settings = {
-    "link_type": "link_only",
-    "blocking_rules_to_generate_predictions": [
+settings = SettingsCreator(
+    link_type="link_only",
+    blocking_rules_to_generate_predictions=[
         block_on("first_name"),
         block_on("surname"),
     ],
-    "comparisons": [
-        ctl.name_comparison("first_name",),
-        ctl.name_comparison("surname"),
-        ctl.date_comparison("dob", cast_strings_to_date=True),
-        cl.exact_match("city", term_frequency_adjustments=True),
-        ctl.email_comparison("email", include_username_fuzzy_level=False),
-    ],
-}
+    comparisons=[
+        cl.NameComparison(
+            "first_name",
+        ),
+        cl.NameComparison("surname"),
+        cl.DateOfBirthComparison(
+            "dob",
+            input_is_string=True,
+            invalid_dates_as_null=True,
+        ),
+        cl.ExactMatch("city").configure(term_frequency_adjustments=True),
+        cl.EmailComparison("email"),
+    ],
+)
+
+linker = Linker(
+    [df_l, df_r],
+    settings,
+    db_api=DuckDBAPI(),
+    input_table_aliases=["df_left", "df_right"],
+)
+
+from splink.exploratory import completeness_chart

-linker = DuckDBLinker([df_l, df_r], settings, input_table_aliases=["df_left", "df_right"])
+completeness_chart(
+    [df_l, df_r],
+    cols=["first_name", "surname", "dob", "city", "email"],
+    db_api=DuckDBAPI(),
+    table_names_for_chart=["df_left", "df_right"],
+)

-linker.completeness_chart(cols=["first_name", "surname", "dob", "city", "email"])

 deterministic_rules = [
     "l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1",
     "l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1",
     "l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2",
-    "l.email = r.email"
+    block_on("email"),
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)

+linker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)

-linker.estimate_u_using_random_sampling(max_pairs=1e6, seed=1)
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6, seed=1)

-session_dob = linker.estimate_parameters_using_expectation_maximisation(block_on("dob"))
-session_email = linker.estimate_parameters_using_expectation_maximisation(block_on("email"))
-session_first_name = linker.estimate_parameters_using_expectation_maximisation(block_on("first_name"))
+session_dob = linker.training.estimate_parameters_using_expectation_maximisation(block_on("dob"))
+session_email = linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("email")
+)
+session_first_name = linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("first_name")
+)

-results = linker.predict(threshold_match_probability=0.9)
+results = linker.inference.predict(threshold_match_probability=0.9)

 results.as_pandas_dataframe(limit=5)
\ No newline at end of file
diff --git a/notebooks_as_py/pairwise_labels.py b/notebooks_as_py/pairwise_labels.py
index 51929b6..73e3d65 100644
--- a/notebooks_as_py/pairwise_labels.py
+++ b/notebooks_as_py/pairwise_labels.py
@@ -1,65 +1,64 @@
-import pandas as pd
-import altair as alt
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink

 from splink.datasets import splink_dataset_labels
+
 pairwise_labels = splink_dataset_labels.fake_1000_labels

 # Choose labels indicating a match
 pairwise_labels = pairwise_labels[pairwise_labels["clerical_match_score"] == 1]
 pairwise_labels

-from splink.datasets import splink_datasets
+from splink import splink_datasets

 df = splink_datasets.fake_1000
 df.head(2)

-from splink.duckdb.linker import DuckDBLinker
-from splink.duckdb.blocking_rule_library import block_on
-import splink.duckdb.comparison_library as cl
-import splink.duckdb.comparison_template_library as ctl
+import splink.comparison_library as cl
+from splink import DuckDBAPI, Linker, SettingsCreator, block_on

-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
         block_on("first_name"),
         block_on("surname"),
     ],
-    "comparisons": [
-        ctl.name_comparison("first_name"),
-        ctl.name_comparison("surname"),
-        ctl.date_comparison("dob", cast_strings_to_date=True),
-        cl.exact_match("city", term_frequency_adjustments=True),
-        ctl.email_comparison("email", include_username_fuzzy_level=False),
+    comparisons=[
+        cl.NameComparison("first_name"),
+        cl.NameComparison("surname"),
+        cl.DateOfBirthComparison(
+            "dob",
+            input_is_string=True,
+        ),
+        cl.ExactMatch("city").configure(term_frequency_adjustments=True),
+        cl.EmailComparison("email"),
     ],
-    "retain_matching_columns": True,
-    "retain_intermediate_calculation_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)

-linker = DuckDBLinker(df, settings, set_up_basic_logging=False)
+linker = Linker(df, settings, db_api=DuckDBAPI(), set_up_basic_logging=False)
 deterministic_rules = [
     "l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1",
     "l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1",
     "l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2",
-    "l.email = r.email"
+    "l.email = r.email",
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)
-
+linker.training.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)

-linker.estimate_u_using_random_sampling(max_pairs=1e6)
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6)

 # Register the pairwise labels table with the database, and then use it to estimate the m values
-labels_df = linker.register_labels_table(pairwise_labels, overwrite=True)
-linker.estimate_m_from_pairwise_labels(labels_df)
+labels_df = linker.table_management.register_labels_table(pairwise_labels, overwrite=True)
+linker.training.estimate_m_from_pairwise_labels(labels_df)


 # If the labels table already existing in the dataset you could run
-# linker.estimate_m_from_pairwise_labels("labels_tablename_here")
-
+# linker.training.estimate_m_from_pairwise_labels("labels_tablename_here")

 training_blocking_rule = block_on("first_name")
-linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)
+linker.training.estimate_parameters_using_expectation_maximisation(training_blocking_rule)

-linker.parameter_estimate_comparisons_chart()
+linker.visualisations.parameter_estimate_comparisons_chart()

-linker.match_weights_chart()
\ No newline at end of file
+linker.visualisations.match_weights_chart()
\ No newline at end of file
diff --git a/notebooks_as_py/quick_and_dirty_persons.py b/notebooks_as_py/quick_and_dirty_persons.py
index 7ceb7b4..629075d 100644
--- a/notebooks_as_py/quick_and_dirty_persons.py
+++ b/notebooks_as_py/quick_and_dirty_persons.py
@@ -1,40 +1,56 @@
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
 from splink.datasets import splink_datasets
+
 df = splink_datasets.historical_50k
 df.head(5)

-from splink.duckdb.linker import DuckDBLinker
-from splink.duckdb.blocking_rule_library import block_on
-import splink.duckdb.comparison_library as cl
+from splink import block_on, SettingsCreator
+import splink.comparison_library as cl

-settings = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
+
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
         block_on("full_name"),
-        block_on(["substr(full_name,1,6)", "dob", "birth_place"]),
-        block_on(["dob", "birth_place"]),
+        block_on("substr(full_name,1,6)", "dob", "birth_place"),
+        block_on("dob", "birth_place"),
         block_on("postcode_fake"),
     ],
-    "comparisons": [
-        cl.jaro_at_thresholds("full_name", [0.9, 0.7], term_frequency_adjustments=True),
-        cl.levenshtein_at_thresholds("dob", [1, 2]),
-        cl.levenshtein_at_thresholds("postcode_fake", 2),
-        cl.jaro_winkler_at_thresholds("birth_place", 0.9, term_frequency_adjustments=True),
-        cl.exact_match("occupation",  term_frequency_adjustments=True),
-    ],
-
-}
-
-linker = DuckDBLinker(df, settings, set_up_basic_logging=False)
+    comparisons=[
+        cl.ForenameSurnameComparison(
+            "first_name",
+            "surname",
+            forename_surname_concat_col_name="first_and_surname",
+        ),
+        cl.DateOfBirthComparison(
+            "dob",
+            input_is_string=True,
+        ),
+        cl.LevenshteinAtThresholds("postcode_fake", 2),
+        cl.JaroWinklerAtThresholds("birth_place", 0.9).configure(
+            term_frequency_adjustments=True
+        ),
+        cl.ExactMatch("occupation").configure(term_frequency_adjustments=True),
+    ],
+)
+
+from splink import Linker, DuckDBAPI
+
+
+linker = Linker(df, settings, db_api=DuckDBAPI(), set_up_basic_logging=False)
 deterministic_rules = [
     "l.full_name = r.full_name",
     "l.postcode_fake = r.postcode_fake and l.dob = r.dob",
 ]

-linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.6)
-
+linker.training.estimate_probability_two_random_records_match(
+    deterministic_rules, recall=0.6
+)

-linker.estimate_u_using_random_sampling(max_pairs=2e6)
+linker.training.estimate_u_using_random_sampling(max_pairs=2e6)

-results = linker.predict(threshold_match_probability=0.9)
+results = linker.inference.predict(threshold_match_probability=0.9)

 results.as_pandas_dataframe(limit=5)
\ No newline at end of file
diff --git a/notebooks_as_py/real_time_record_linkage.py b/notebooks_as_py/real_time_record_linkage.py
index a05990f..61e92b1 100644
--- a/notebooks_as_py/real_time_record_linkage.py
+++ b/notebooks_as_py/real_time_record_linkage.py
@@ -1,52 +1,65 @@
-import json
-from splink.datasets import splink_datasets
-from splink.duckdb.linker import DuckDBLinker
-import altair as alt
-alt.renderers.enable('html')
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install ipywidgets
+# !pip install splink
+# !jupyter nbextension enable --py widgetsnbextension

-with open("../../demo_settings/real_time_settings.json") as f:
-    trained_settings = json.load(f)
+import urllib.request
+import json
+from pathlib import Path
+from splink import Linker, DuckDBAPI, block_on, SettingsCreator, splink_datasets

 df = splink_datasets.fake_1000

-linker = DuckDBLinker(df, trained_settings)
+url = "https://raw.githubusercontent.com/moj-analytical-services/splink_demos/master/demo_settings/real_time_settings.json"
+
+with urllib.request.urlopen(url) as u:
+    settings = json.loads(u.read().decode())
+
+
+linker = Linker(df, settings, db_api=DuckDBAPI())

-linker.waterfall_chart(linker.predict().as_record_dict(limit=2))
+linker.visualisations.waterfall_chart(
+    linker.inference.predict().as_record_dict(limit=2)
+)

-from splink.term_frequencies import compute_term_frequencies_from_concat_with_tf
-record_1  = {
-     'unique_id':1,
-     'first_name': "Lucas",
-     'surname': "Smith",
-     'dob': "1984-01-02",
-     'city': "London",
-     'email': "lucas.smith@hotmail.com"
+record_1 = {
+    "unique_id": 1,
+    "first_name": "Lucas",
+    "surname": "Smith",
+    "dob": "1984-01-02",
+    "city": "London",
+    "email": "lucas.smith@hotmail.com",
 }

-record_2  = {
-     'unique_id':2,
-     'first_name': "Lucas",
-     'surname': "Smith",
-     'dob': "1983-02-12",
-     'city': "Machester",
-     'email': "lucas.smith@hotmail.com"
+record_2 = {
+    "unique_id": 2,
+    "first_name": "Lucas",
+    "surname": "Smith",
+    "dob": "1983-02-12",
+    "city": "Machester",
+    "email": "lucas.smith@hotmail.com",
 }

-linker._settings_obj_._retain_intermediate_calculation_columns = True
-linker._settings_obj_._retain_matching_columns = True
+linker._settings_obj._retain_intermediate_calculation_columns = True

-linker.compute_tf_table("first_name")
-linker.compute_tf_table("surname")
-linker.compute_tf_table("dob")
-linker.compute_tf_table("city")
-linker.compute_tf_table("email")

+# To `compare_two_records` the linker needs to compute term frequency tables
+# If you have precomputed tables, you can linker.register_term_frequency_lookup()
+linker.table_management.compute_tf_table("first_name")
+linker.table_management.compute_tf_table("surname")
+linker.table_management.compute_tf_table("dob")
+linker.table_management.compute_tf_table("city")
+linker.table_management.compute_tf_table("email")

-df_two = linker.compare_two_records(record_1, record_2)
+
+df_two = linker.inference.compare_two_records(record_1, record_2)
 df_two.as_pandas_dataframe()

 import ipywidgets as widgets
-fields = ["unique_id", "first_name","surname","dob","email","city"]
+from IPython.display import display
+
+
+fields = ["unique_id", "first_name", "surname", "dob", "email", "city"]

 left_text_boxes = []
 right_text_boxes = []
@@ -54,87 +67,90 @@
 inputs_to_interactive_output = {}

 for f in fields:
-    wl = widgets.Text(description=f, value =str(record_1[f]))
+    wl = widgets.Text(description=f, value=str(record_1[f]))
     left_text_boxes.append(wl)
     inputs_to_interactive_output[f"{f}_l"] = wl
-    wr = widgets.Text( description=f, value =str(record_2[f]))
+    wr = widgets.Text(description=f, value=str(record_2[f]))
     right_text_boxes.append(wr)
     inputs_to_interactive_output[f"{f}_r"] = wr

-
 b1 = widgets.VBox(left_text_boxes)
 b2 = widgets.VBox(right_text_boxes)
-ui = widgets.HBox([b1,b2])
+ui = widgets.HBox([b1, b2])
+

 def myfn(**kwargs):
     my_args = dict(kwargs)
-
+
     record_left = {}
     record_right = {}
-
+
     for key, value in my_args.items():
-        if value == '':
+        if value == "":
             value = None
         if key.endswith("_l"):
             record_left[key[:-2]] = value
-        if key.endswith("_r"):
+        elif key.endswith("_r"):
             record_right[key[:-2]] = value
-

-    linker._settings_obj_._retain_intermediate_calculation_columns = True
-    linker._settings_obj_._retain_matching_columns = True
+    # Assuming 'linker' is defined earlier in your code
+    linker._settings_obj._retain_intermediate_calculation_columns = True

-    df_two = linker.compare_two_records(record_left, record_right)
+    df_two = linker.inference.compare_two_records(record_left, record_right)

     recs = df_two.as_pandas_dataframe().to_dict(orient="records")
-    from splink.charts import waterfall_chart
-    display(linker.waterfall_chart(recs, filter_nulls=False))

+    display(linker.visualisations.waterfall_chart(recs, filter_nulls=False))

-out = widgets.interactive_output(myfn, inputs_to_interactive_output)

-display(ui,out)
+out = widgets.interactive_output(myfn, inputs_to_interactive_output)

+display(ui, out)

-record = {'unique_id': 123987,
- 'first_name': "Robert",
- 'surname': "Alan",
- 'dob': "1971-05-24",
- 'city': "London",
- 'email': "robert255@smith.net"
+record = {
+    "unique_id": 123987,
+    "first_name": "Robert",
+    "surname": "Alan",
+    "dob": "1971-05-24",
+    "city": "London",
+    "email": "robert255@smith.net",
 }


-
-df_inc = linker.find_matches_to_new_records([record], blocking_rules=[]).as_pandas_dataframe()
+df_inc = linker.inference.find_matches_to_new_records(
+    [record], blocking_rules=[]
+).as_pandas_dataframe()
 df_inc.sort_values("match_weight", ascending=False)

-from splink.charts import waterfall_chart
-
-@widgets.interact(first_name='Robert', surname="Alan", dob="1971-05-24", city="London", email="robert255@smith.net")
-def interactive_link(first_name, surname, dob, city, email):
-
-    record = {'unique_id': 123987,
-     'first_name': first_name,
-     'surname': surname,
-     'dob': dob,
-     'city': city,
-     'email': email,
-     'group': 0}
+@widgets.interact(
+    first_name="Robert",
+    surname="Alan",
+    dob="1971-05-24",
+    city="London",
+    email="robert255@smith.net",
+)
+def interactive_link(first_name, surname, dob, city, email):
+    record = {
+        "unique_id": 123987,
+        "first_name": first_name,
+        "surname": surname,
+        "dob": dob,
+        "city": city,
+        "email": email,
+        "group": 0,
+    }

     for key in record.keys():
         if type(record[key]) == str:
             if record[key].strip() == "":
                 record[key] = None

-
-    df_inc = linker.find_matches_to_new_records([record], blocking_rules=[f"(true)"]).as_pandas_dataframe()
+    df_inc = linker.inference.find_matches_to_new_records(
+        [record], blocking_rules=[f"(true)"]
+    ).as_pandas_dataframe()
     df_inc = df_inc.sort_values("match_weight", ascending=False)
     recs = df_inc.to_dict(orient="records")
-
-
-
-    display(linker.waterfall_chart(recs, filter_nulls=False))

+    display(linker.visualisations.waterfall_chart(recs, filter_nulls=False))

-linker.match_weights_chart()
\ No newline at end of file
+linker.visualisations.match_weights_chart()
\ No newline at end of file
diff --git a/notebooks_as_py/transactions.py b/notebooks_as_py/transactions.py
index 79ed254..2fd3f79 100644
--- a/notebooks_as_py/transactions.py
+++ b/notebooks_as_py/transactions.py
@@ -1,6 +1,7 @@
-from splink.datasets import splink_datasets
-from splink.duckdb.linker import DuckDBLinker
-import altair as alt
+# Uncomment and run this cell if you're running in Google Colab.
+# !pip install splink
+
+from splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets

 df_origin = splink_datasets.transactions_origin
 df_destination = splink_datasets.transactions_destination
@@ -8,14 +9,25 @@
 display(df_origin.head(2))
 display(df_destination.head(2))

-# Simple settings just for exploratory analysis
-settings = {"link_type": "link_only"}
-linker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=["__ori", "_dest"])
-linker.profile_columns(["transaction_date", "memo", "round(amount/5, 0)*5"])
+from splink.exploratory import profile_columns

-# Design blocking rules that allow for differences in transaction date and amounts
-from splink.duckdb.blocking_rule_library import block_on, and_
+db_api = DuckDBAPI()
+profile_columns(
+    [df_origin, df_destination],
+    db_api=db_api,
+    column_expressions=[
+        "memo",
+        "transaction_date",
+        "amount",
+    ],
+)

+from splink import DuckDBAPI, block_on
+from splink.blocking_analysis import (
+    cumulative_comparisons_to_be_scored_from_blocking_rules_chart,
+)
+
+# Design blocking rules that allow for differences in transaction date and amounts
 blocking_rule_date_1 = """
     strftime(l.transaction_date, '%Y%m') = strftime(r.transaction_date, '%Y%m')
     and substr(l.memo, 1,3) = substr(r.memo,1,3)
@@ -42,81 +54,124 @@
 blocking_rule_cheat = block_on("unique_id")


-linker.cumulative_num_comparisons_from_blocking_rules_chart(
-    [
-        blocking_rule_date_1,
-        blocking_rule_date_2,
-        blocking_rule_memo,
-        blocking_rule_amount_1,
-        blocking_rule_amount_2,
-        blocking_rule_cheat,
-    ]
+brs = [
+    blocking_rule_date_1,
+    blocking_rule_date_2,
+    blocking_rule_memo,
+    blocking_rule_amount_1,
+    blocking_rule_amount_2,
+    blocking_rule_cheat,
+]
+
+
+db_api = DuckDBAPI()
+
+cumulative_comparisons_to_be_scored_from_blocking_rules_chart(
+    table_or_tables=[df_origin, df_destination],
+    blocking_rules=brs,
+    db_api=db_api,
+    link_type="link_only"
 )

 # Full settings for linking model
-import splink.duckdb.comparison_library as cl
-import splink.duckdb.comparison_level_library as cll
+import splink.comparison_level_library as cll
+import splink.comparison_library as cl

 comparison_amount = {
     "output_column_name": "amount",
     "comparison_levels": [
-        cll.null_level("amount"),
-        cll.exact_match_level("amount"),
-        cll.percentage_difference_level("amount",0.01),
-        cll.percentage_difference_level("amount",0.03),
-        cll.percentage_difference_level("amount",0.1),
-        cll.percentage_difference_level("amount",0.3),
-        cll.else_level()
+        cll.NullLevel("amount"),
+        cll.ExactMatchLevel("amount"),
+        cll.PercentageDifferenceLevel("amount", 0.01),
+        cll.PercentageDifferenceLevel("amount", 0.03),
+        cll.PercentageDifferenceLevel("amount", 0.1),
+        cll.PercentageDifferenceLevel("amount", 0.3),
+        cll.ElseLevel(),
     ],
     "comparison_description": "Amount percentage difference",
 }

-settings = {
-    "link_type": "link_only",
-    "probability_two_random_records_match": 1 / len(df_origin),
-    "blocking_rules_to_generate_predictions": [
+# The date distance is one sided becaause transactions should only arrive after they've left
+# As a result, the comparison_template_library date difference functions are not appropriate
+within_n_days_template = "transaction_date_r - transaction_date_l <= {n} and transaction_date_r >= transaction_date_l"
+
+comparison_date = {
+    "output_column_name": "transaction_date",
+    "comparison_levels": [
+        cll.NullLevel("transaction_date"),
+        {
+            "sql_condition": within_n_days_template.format(n=1),
+            "label_for_charts": "1 day",
+        },
+        {
+            "sql_condition": within_n_days_template.format(n=4),
+            "label_for_charts": "<=4 days",
+        },
+        {
+            "sql_condition": within_n_days_template.format(n=10),
+            "label_for_charts": "<=10 days",
+        },
+        {
+            "sql_condition": within_n_days_template.format(n=30),
+            "label_for_charts": "<=30 days",
+        },
+        cll.ElseLevel(),
+    ],
+    "comparison_description": "Transaction date days apart",
+}
+
+
+settings = SettingsCreator(
+    link_type="link_only",
+    probability_two_random_records_match=1 / len(df_origin),
+    blocking_rules_to_generate_predictions=[
         blocking_rule_date_1,
         blocking_rule_date_2,
         blocking_rule_memo,
         blocking_rule_amount_1,
         blocking_rule_amount_2,
-        blocking_rule_cheat
+        blocking_rule_cheat,
     ],
-    "comparisons": [
+    comparisons=[
         comparison_amount,
-        cl.jaccard_at_thresholds(
-            "memo", [0.9, 0.7]
-        ),
-        cl.datediff_at_thresholds("transaction_date",
-                                date_thresholds = [1, 4, 10, 30],
-                                date_metrics = ["day", "day", "day", "day"],
-                                include_exact_match_level=False
-                                )
+        cl.LevenshteinAtThresholds("memo", [2, 6, 10]),
+        comparison_date,
     ],
-    "retain_intermediate_calculation_columns": True,
-    "retain_matching_columns": True,
-}
+    retain_intermediate_calculation_columns=True,
+)

-linker = DuckDBLinker([df_origin, df_destination], settings,input_table_aliases=["__ori", "_dest"])
+linker = Linker(
+    [df_origin, df_destination],
+    settings,
+    input_table_aliases=["__ori", "_dest"],
+    db_api=db_api,
+)

-linker.estimate_u_using_random_sampling(max_pairs=1e6)
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6)

-linker.estimate_parameters_using_expectation_maximisation(block_on("memo"))
+linker.training.estimate_parameters_using_expectation_maximisation(block_on("memo"))

-session = linker.estimate_parameters_using_expectation_maximisation(block_on("amount"))
+session = linker.training.estimate_parameters_using_expectation_maximisation(block_on("amount"))

-linker.match_weights_chart()
+linker.visualisations.match_weights_chart()

-df_predict = linker.predict(threshold_match_probability=0.001)
+df_predict = linker.inference.predict(threshold_match_probability=0.001)

-linker.comparison_viewer_dashboard(df_predict,"dashboards/comparison_viewer_transactions.html", overwrite=True)
+linker.visualisations.comparison_viewer_dashboard(
+    df_predict, "dashboards/comparison_viewer_transactions.html", overwrite=True
+)
 from IPython.display import IFrame
+
 IFrame(
     src="./dashboards/comparison_viewer_transactions.html", width="100%", height=1200
 )

-pred_errors =  linker.prediction_errors_from_labels_column("ground_truth", include_false_positives=True, include_false_negatives=False)
-linker.waterfall_chart(pred_errors.as_record_dict(limit=5))
+pred_errors = linker.evaluation.prediction_errors_from_labels_column(
+    "ground_truth", include_false_positives=True, include_false_negatives=False
+)
+linker.visualisations.waterfall_chart(pred_errors.as_record_dict(limit=5))

-pred_errors =  linker.prediction_errors_from_labels_column("ground_truth", include_false_positives=False, include_false_negatives=True)
-linker.waterfall_chart(pred_errors.as_record_dict(limit=5))
\ No newline at end of file
+pred_errors = linker.evaluation.prediction_errors_from_labels_column(
+    "ground_truth", include_false_positives=False, include_false_negatives=True
+)
+linker.visualisations.waterfall_chart(pred_errors.as_record_dict(limit=5))
\ No newline at end of file
diff --git a/script.py b/script.py
index 465b599..306222c 100644
--- a/script.py
+++ b/script.py
@@ -1,51 +1,44 @@
-import splink.duckdb.comparison_library as cl
-from splink.datasets import splink_datasets
-from splink.duckdb.blocking_rule_library import block_on
-from splink.duckdb.linker import DuckDBLinker
+import splink.comparison_library as cl
+from splink import DuckDBAPI, Linker, SettingsCreator, block_on, splink_datasets

 df = splink_datasets.historical_50k

-settings_dict = {
-    "link_type": "dedupe_only",
-    "blocking_rules_to_generate_predictions": [
-        block_on(["postcode_fake", "first_name"]),
-        block_on(["first_name", "surname"]),
-        block_on(["dob", "substr(postcode_fake,1,2)"]),
+settings = SettingsCreator(
+    link_type="dedupe_only",
+    blocking_rules_to_generate_predictions=[
+        block_on("postcode_fake", "first_name"),
+        block_on("first_name", "surname"),
+        block_on("dob", "substr(postcode_fake,1,2)"),
     ],
-    "comparisons": [
-        cl.exact_match(
-            "first_name",
+    comparisons=[
+        cl.ExactMatch("first_name").configure(
             term_frequency_adjustments=True,
         ),
-        cl.jaro_winkler_at_thresholds(
-            "surname",
-            distance_threshold_or_thresholds=[0.9, 0.8],
-        ),
-        cl.levenshtein_at_thresholds("postcode_fake"),
-        cl.levenshtein_at_thresholds("dob"),
+        cl.JaroWinklerAtThresholds("surname", score_threshold_or_thresholds=[0.9, 0.8]),
+        cl.LevenshteinAtThresholds("postcode_fake"),
     ],
-}
-
+)

-linker = DuckDBLinker(df, settings_dict)
+linker = Linker(df, settings, db_api=DuckDBAPI())

-linker.estimate_probability_two_random_records_match(
+linker.training.estimate_probability_two_random_records_match(
     deterministic_matching_rules=[
-        block_on(["first_name", "surname"]),
-        block_on(["dob", "postcode_fake"]),
+        block_on("first_name", "surname"),
+        block_on("dob", "postcode_fake"),
     ],
     recall=0.7,
 )

-linker.estimate_u_using_random_sampling(max_pairs=1e6)
+linker.training.estimate_u_using_random_sampling(max_pairs=1e6)

-linker.estimate_parameters_using_expectation_maximisation(
-    block_on(["first_name", "surname"])
+linker.training.estimate_parameters_using_expectation_maximisation(
+    block_on("first_name", "surname")
 )

+linker.training.estimate_parameters_using_expectation_maximisation(block_on("dob"))

-df_predict = linker.predict()
+df_predict = linker.inference.predict()

-linker.cluster_pairwise_predictions_at_threshold(
-    df_predict, threshold_match_probability=0.5
+linker.clustering.cluster_pairwise_predictions_at_threshold(
+    df_predict, threshold_match_probability=0.9
 )